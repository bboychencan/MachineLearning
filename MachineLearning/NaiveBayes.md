# NaiveBayes

朴素贝叶斯分类算法，应该是最早听说的算法之一，名字里有朴素两个字，一直觉得很简答，不就是贝叶斯原理吗，P(Y|X) = P(X, Y) / P(X)

可是似乎一直没有彻底的理解这个算法，而且这个算法属于是生成模型，跟其他的LR，SVM属于不同性质的算法。 一直以来没能完全搞清楚。

这次决定给弄清楚，以后就不会再稀里糊涂了。

## 含义

贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。而朴素朴素贝叶斯分类是贝叶斯分类中最简单，也是常见的一种分类方法

其实理解起来很简单，就是在训练样本集中，可以估计出P(Y=Ck)的概率，也可以估计出P(x1, x2, x3, .. xn, Yi)的概率。

最直接的想法，当遇到一个观测样本X=(x1', x2', ... xn')时，最直接的想法，直接计算P(x1', x2', ... xn', Y1)和
P(x1', x2', ... xn', Y2)然后取较大的作为最后的预测。
这个在样本维度比较小的时候是完全可行的，也很符合直觉。

但是这里有个问题，如果X的维度很大，而且每一维的取值都很多的情况下，直接计算P(x1, x2, x3, .. xn, Yi)时，很可能会遇到数据非常稀疏的问题。
直白来讲，即使你见过很多人，但是再见到一个和之前完全一样的人也是不可能的，因为世界上就没有完全一样的人，如果这样的话那么P(x1, x2, x3, .. xn, Yi)
直接就为0，没有意义。 

所以朴素贝叶斯算法就做了个假设，所以的P(Xi|Y)是独立的，也即条件独立假设。 
这样的话计算P(x1, x2, x3, .. xn, Yi)就不需要直接找到完全一样的一个人，而是从各个维度去计算，P(X1) * P(X2) * .. P(Xn) P(Yi)，这样就可以
计算了。

而P(Xi) 和P(Yi)之间肯定不是独立的，不然就没有意义。。。 完全就是选P(Yi)大的就可以了，因为如果假设X和Y是独立的，那么P(Y|X)直接就等于P(Y)。。。
那还算个毛线。。。 所以的话肯定是要求条件概率。。

ok 那么P(x1, x2, x3, .. xn, Yi) 没法直接计算了，但是我有条件独立的假设，所以可以计算这个值

P(x1, x2, x3, .. xn | Yi) = P(X1|Yi) * P(X2|Yi) ... 

就可惜没有一个公式 P(Y|X) = P(Y|X1) * P(Y|X2) * P(Y|X3) ... 不然世界就简单了。。。

不过反过来的公式倒是有， P(X1|Y) * P(X2|Y) ... = P(X1, X2, .. Xn | Y)

然后利用贝叶斯公式 P(Y|X) = P(X, Y) / P(X)
P(X, Y) = P(X1, X2, .. Xn | Y) * P(Y) 
这样，这几个值都可以从样本中估计出来，所以所有的问题就此解决了。

## 生成模型
目前我的认知是，生成模型需要学习整个P(X, Y)而判别模型只需要学习P(Y|X)即可。

