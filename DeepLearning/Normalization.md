# Normalization 归一化/标准化
2022-01-06
## 为什么做归一化
如果一个机器学习算法在缩放全部或部分特征之后不影响它的学习和预测，我们就称该算法有尺度不变形(scale invariance)，如
线性分类器就是尺度不变的。 而最进邻分类器这种需要计算欧式距离的就是尺度敏感的。

理论上来讲，神经网络应该是具有尺度不变性，可以通过参数调整适应不同的尺度。
但是尺度不同的输入特征会增加训练难度。 例如以tanh做激活函数为例，只有在[-2, 2]区间上是敏感的，其余区域上导数都接近
与0，如果wx + b 过大或过小，都会导致导数梯度过小，难以训练。
而为了提高训练效率，可以调整w使得 wx+b到[-2,2]区间，但是如果数据维数很多时，很难精心选择每一个参数。

数据经过归一化和标准化后可以加快梯度下降的求解速度。 使得可以使用更大的学习率更稳定地进行梯度传播，
甚至增加网络的泛化能力。，但是如果数据维数很多时，很难精心选择每一个参数。

归一化是统计学里常见的概念，目的是对数据进行偏移和尺度缩放调整

## 常见的归一化方法
- 线性归一化   X = (x - xmin) / (xmax - xmin)
- 零均值归一化/Z-score 标准化 yi = (xi - mu) / sigma 处理后变成均值为0，标准差为1的分布
上面两种方法是线性变换，**不会**改变分布本身的形状

## Internal Covariate Shift 内部协变量偏移
先理解什么是covariate shift。
其实就是在一般的机器学习中，都做了一个假设，就是训练集/测试集的数据是满足独立同分布IID的。这样利用最大似然估计
从训练集中训练出来的参数theta才能适用于测试集。 
但是当现实中，伴随这新数据产生， 旧数据过期，训练集和测试集的数据不再满足IID，被称作covariate shif

**dataset shift**
训练数据和待预测数据分布不同，称为数据集偏移，如果训练样本容量小，使得采集到的训练样本不能反应真实世界特征，就会
导致dataset shift

**covariate shift** 是一种特殊的dataset shift。 输入变量x是解释变量explanatory variable或协变量covariate，类别
标签y是应变量response variable。 
如果训练数据和待预测数据条件概率相同，但边缘概率不同
- Ptr(y|x) = Pte(y|x)
- Ptr(x) != Pte(x)
这种情况就被称为covariate shift
而且根据条件概率定义，推出训练数据和待预测数据的联合概率不同，所以训练出来的模型就出现偏差

在深度学习的场景下，上一层的输出完全决定了之后结果的产生。 假设在某一轮迭代中，基于上一轮的输出得到了之后每一层
参数的更新，然而在下一轮训练中，由于上一层参数自身也会产生更新，导致上一轮的输出发生了一些变化，分布也有了变化。
那么单从上一层的输出来看剩下整个模型的角度来看。 前一轮训练的结果势必就不适用于新一轮训练的数据，导致更新梯度的
时候就会发生偏差。 这个就被叫做Internal Covaraite Shift。

采取的解决方案即对每一层都做normalization，保证每一层输出的分布是不变的，然后避免了ICS的问题。

但是，实际想想，是不是均值方差一致就是相同的分布了呢？其实不一定，normalization这种方式，实际上并不是直接去解
决ICS问题，更多的是面向梯度消失等，去加速网络收敛的。类似covariance shift比较直接的解决思路应该是对样本的Reweight
操作，根据前后两个分布进行权重的学习，再对新的分布进行reweight


## Batch Norm
为什么做batch norm。主要是为了客服深度神经网络难以训练的弊病。

基本原理
batch norm中的batch是每次处理的批量数据,

- BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。
- 另外逐层归一化，使得大部分神经层的输入处于不饱和区域，从而让梯度变大，避免梯度消失的问题。使得梯度变得
更加稳定，允许我们适用更大的学习率。- 

一些细节，如果使用sigmoid类的激活函数，那么归一化后的取值区间刚好是接近线性变换的区间，减弱了神经网络的非线性
性质，为了使归一化不对网络造成负面影响，可以添加附加的缩放gamma和平移beta，这两个参数也是需要学习的

BN可以起到正则化的作用，所以在正则化方面，一般全连接层用dropout，卷积层用BN。
## Batch Norm (BN) 与 Layer所以Norm (LN)
- BN是对单个神经元的输入做归一，取的是batch内不同样本的输入z来做归一，个数是batch size
- LY是对一层神经元做归一，但是每次只用一个训练数据，一个训练数据得到某一层M个神经元的所有输入z，然后做归一
- BN要求batch size不能太小，否则很难统计。

