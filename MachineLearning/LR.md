# LR 推导 

逻辑回归是一个非常经典的算法，其中也包含了非常多的细节，曾看到一句话：如果面试官问你熟悉哪个机器学习模型，可以说 SVM，但千万别说 LR，因为细节真的太多了!!! 果不其然，挂在这了！！ 

还是理解的不够深入彻底，在这里把所有的细节一一列举出来，防止以后再跪在这里。

其实就是这样一个问题，在工程实践中会遇到很多的困难，很多的细节，很可能需要都踩过一遍才能透彻理解。

比如说面试官追问的一些问题
1. 首先是手动把损失函数偏导求出
2. 如果说每次迭代把所有的样本全部计算一遍，这样是不是成本太大（关于SGD）
3. 一阶的偏导会有什么缺点？ （正则化，岭回归，Lasso等）
4. 学习率怎么定，（Adam，等）
5. 

总结一下就是深入去搞模型的话，会遇到很多这样那样的坑，只有踩过一遍才知道为什么会这样做那样做。
而且，上面几个问题，在统计学习方法里面都没有提到。。。尤其是LR那一章


# 最大熵原理

直观上理解很简单，就是在对未知的事物作判断是，假设完全随机，符合均匀分布（即熵最大）。而已知的条件要满足，也就是条件约束

# 拉格朗日乘子法

在约束条件下，求函数极值。 在多元微积分里面经常使用，求解约束条件下的最优解