# Newton method

这个细节竟然被问到，然后没有理解完全透彻，感觉很sb。。

还有海森矩阵。。。 感觉统计学的也不是很扎实


牛顿法（Newton method）和拟牛顿法（quasi Newton method）是求解无约束最优化问题的常用方法，有收敛速度快的优点


## 牛顿法原理
这个看了一下，发现就是大学时候学的内容，就是定一个初始点，取切线与x坐标的交点得到新的x1，然后一次迭代下去就可以找到f(x) = 0的解

## 收敛性证明


## 牛顿法用来解最优化问题
对于无约束最优化问题min(f(x))，根据极小点必要条件导数(f(x)) = 0，然后采用牛顿法求解

对于一维的x来说，很容易理解，对于多维x向量，这里面的一阶梯度是一个向量，二阶梯度是矩阵形式，分别记为g和H（Hessian矩阵）

于是求f(x)的极值就转换为求f'(x) = 0的值，然后带入刚才的两个记号
gk + Hk(x-xk) = 0 若Hk非奇异，x = xk - Hk^-1 * gk 于是可以迭代计算了

这个就是原始的牛顿法，由于每次都要计算Hk的逆家族恶化呢，计算复杂度较高。

当原函数为二次函数时，二阶泰勒展开就不再是近似而是和原函数一样的二次式，H矩阵退化为常数矩阵。 所以牛顿法是一种具有二次收敛性的算法。
对于非二次函数，若函数的二次性态较强，则其收敛速度也很快。

## 拟牛顿法
由于每次迭代都需要计算H逆矩阵，而有时H矩阵无法保持正定，导致牛顿法失效，为了克服这两个问题，提出了拟牛顿法。
即不用二阶偏导数而构造出近似H矩阵的正定对称阵。不同的构造方法就产生了不同的拟牛顿法。
这里面有DFP算法，BFGS算法，L-BFGS算法

## 与梯度下降对比

本质上看，牛顿法是二阶收敛，梯度下降是一阶收敛
