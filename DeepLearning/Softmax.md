# Softmax

2021-12-28
对于二分类问题，已经很熟悉，使用sigmoid函数将实数空间映射到[0, 1]区间。 
softmax的话指的是多分类时使用的“函数”。 具体细节很久没有用过，所以有些记不清楚了。


## 原理
softmax 函数，又成为“归一化指数函数”，是二分类sigmoid在多分类的推广指数
刚才又看了一下定义，实在是太简单了。其实就是为了实现概率函数的两种性质
1. 非负
2. 各个类别概率和为1

具体做的就是
1. 使用指数函数exp(x)将实数值区间映射到[0, +无穷) 满足了非负性
2. 为了保证类别概率和为1。每个类别的输出映射到非负区间后 exp(xi)，将所有类别的输出值
求和做分母，分子就是每个类别的输出值，比例就是概率。 求和就是1

其实很容易可以发现二分类的sigmoid是softmax的一种特例

## 名字由来
先了解softmax函数和softmax激活函数是不同的概念
softmax函数 形如  ln(sum(exp(xi))) 

这里可以看到softmax函数其实是max的一个近似，想象一下max(0, x) 和 softmax(0, x)在二维平面的曲线形状，
max是一个在零点不可导的曲线，而softmax相对就比较平滑，而且跟max曲线很近似

softmax激活函数，也就是我们上面讨论的将数值转换成多分类概率向量的函数，跟softmax函数不一样。
他其实是长得有点像softmax函数函数

## 损失函数
其实截止到目前看来，如果只判断最终的输出是哪一类，那么其实似乎做不做归一都一样，只需要选概率最大
的即可，但是如果考虑到损失的化，那么就像二分类里面一样，会计算真实值（0或1）与实际值（概率值p）
之间的误差，那么这时候输出的是hardmax还是softmax就有区别了。

这里面的损失函数依然沿用交叉熵的形式。 具体的交叉熵求导，较为复杂，暂且按下不表
