# Activation 激活函数

## sigmoid 函数
最基本的将实数域映射到0，1区间

1. 实现了归一化，输出在0，1之间
2. 适合用于预测概率作为输出的模型，因为概率的取值范围也是0到1
3. 梯度平滑，避免了跳跃输出值

缺点
1. 指数运算，计算机运行较慢


## Tanh 双曲正切激活函数

1. 跟sigmoid类似，当输入较大或较小时，输出几乎是平滑的且梯度较小。
2. 输出间隔是-1到1


## ReLU
深度学习较为流行的一种激活函数
max(0, x)
1. 输入为正时，不存在梯度饱和问题？
2. 计算速度快，只有线性关系
3. 输入为负时，ReLU完全失效



## softmax激活

