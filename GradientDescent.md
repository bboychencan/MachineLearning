# Gradient Descent

梯度下降是解优化问题的重要手段，这里整理一下常见的梯度下降方法，除了最基础的梯度下降外还有一些拓展如SGD，BGD


## GD
一阶导，负梯度即为最速下降方向，也叫最速下降法。
缺点：
1. 靠近极小值时，收敛速度减慢
2. 可能会“之字形”下降

## BGD
如果采用GD，每次需要计算全部训练数据，效率很低。

## SGD
每次通过一个样本来迭代更新一次，

## Mini-batch GD
小批量梯度下降，集合了批量梯度下降和随机梯度下降两者的优势，每次参数更新利用一小批数据完成。
特点：降低了更新参数的方差，是的收敛过程更加稳定

## Momentum动量法
小批量下降的一个缺点在于，更新的方向完全依赖于当前batch计算出的梯度，十分不稳定。
Momentum借用了物理中的动量概念，模拟物体运动时候的惯性，即在更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度
对之前的梯度进行微调。这样依赖，可以增加稳定性，使学习更快，并有一定摆脱局部最优的能力。

梯度下降的迭代公式𝑤=𝑤−𝛼∗𝑑𝑤 其中w是网络参数，alpha时学习率，dw是梯度，这是梯度下降最基本的形式，在此基础上，人们提出了其他很多变种
目标是使得梯度下降收敛更加稳定和迅速。

Momentum的迭代公式如下
𝑣=𝛽𝑣+(1−𝛽)𝑑𝑤 
𝑤=𝑤−𝛼𝑣

这里面dw是原始梯度，而v是用指数加权平均计算出来的梯度，相当于对原始梯度做了一个平滑，然后再用来做梯度下降，其实就是偏移方向的震动， ，保留大方向

## RMSprop

## Adagrad法

## Adam法
