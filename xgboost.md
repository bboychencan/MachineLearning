# Xgboost

学习过GBDT之后，实战中用的更多的是xgboost，具体两者有什么区别和联系，打算研究一下清楚

1. xbg就是gbdt的一种改进的工程实现，底层依然是加法模型，每个基学习器可以是CART也可以是其他
2. gbdt用的是平方损失，优化的是这种形式的min(min(c1) + min(c2))函数。用贪心发寻找最优的分裂，使得平方损失和最小（使用均值）
而xgb有一个泛化的obj优化函数，需要计算一阶导和二阶导，里面损失函数可以自定义（只要能求一阶导，二阶导即可），同时加入了正则项，主要是树的节点个数和每个节点w值的L2范数和
3. xbg同样可以采用贪心的方法，每次优化obj，分裂找obj增益最多的分裂方式，但是这种贪心算法时间复杂度太高。于是xgb采用了一些工程
上的优化。包括加权quantile避免便利所有的分裂点，以及block存储，还有列采样，cache缓存等。
4. xgb对缺失值做了一些优化处理，可以大幅度的提升效率。对于缺失值会尝试分到左分支与右分支，然后选择最优的一侧


## xgb可以并行执行
1. XGBoost的并行，并不是说每棵树可以并行训练，XGBoost本质上仍然采用boosting思想，每棵树训练前需要等前面的树训练完成才能开始训练。
2. XGBoost的并行，指的是特征维度的并行：在训练之前，每个特征按特征值对样本进行预排序，并存储为Block结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。



