# Multi Armed Bandit 多臂老虎机 MAB

2022-01-10
这个是很经典的引入强化学习的问题。 大概描述如下。
有n个老虎机，每个老虎机的奖赏概率分布都不同但是固定不变。 如何在T次尝试中得到最大的回报，或者说
得到最小的Acumulative Regret累计懊悔值。

先按直觉来思考，首先在实验开始时，我们对每个老虎机的概率分布函数一无所知，我们需要做多次尝试（实验）
才能根据实验结果估计其分布函数和期望。 当然这个估计随着实验次数增加而变得更加准确，大数定律。 
但是我们能尝试的次数是有限的，在一个老虎机上尝试过多可以估算的更精准，但有可能会错过其他期望更高
的老虎机。 所以这里面就涉及到两个问题，Explore & Exploit，探索和利用。 
一方面我们希望尽量广地探索这样方便找到报酬率更高的老虎机，同时我们也希望集中资源投入到高回报的老虎机
上从而得到最大的回报。 如何平衡这两部分，有几种已知的解法

## epsilon greed 
取一个概率参数e， 以e的概率去做随机尝试也就是explore，以 1-e的概率去使用已知的最高回报的老虎机。 
这个策略会受到e超参的影响，实验显示，对于固定e值，累计懊悔值与时间成线性关系。且随着e增大，累计懊悔
增长速率就会增大。 
如果将e值指定为随着时间衰减，那么实验显示累计懊悔与时间就变成次线性关系，接近于指数分布，明显优于固定
值的epsilo greed策略

## Upper Confidence Bound UCB 上置信届算法
基于不确定性，如果一个拉杆的不确定性越大，那么就越值得探索。 因为探索之后可能发现奖励很大。然后该
算法引入不确定性度量U(a)，随着一个动作尝试次数增加而减小。 综合考虑现有期望奖励估值和不确定性，核心是
如何估计不确定性。
UCB用到霍夫丁不等式，可以算出期望奖励的上界，于是上置信届算法便取期望奖励上界最大的动作。 
实验结果显示效果最好，对数形式增长

## Thompson Sampling 汤普森采样
先假设每个拉杆的奖励服从一个特定的分布，然后根据每个拉杆的期望奖励来选择。 直接计算期望代价较高，汤普森
采样是计算每个拉杆产生最高奖励概率的蒙特卡洛采样方法。



