# Embedding


2021-12-19
embedding的概念在深度学习中可以说是最基础，最重要的概念。一直以来没有认真仔细地学习过这个，今天要研究清楚这个概念。


简单来讲，embedding就是用一个低纬的**数字**向量来表达object，非常类似推荐系统里面的矩阵分解算法给每一个user或者item
计算一个隐向量。 然后用这个隐向量来计算user或者item之间的距离。

这一步解决的是”将现实问题转化为数学问题“，是人工智能非常关键的一步。


矩阵分解里面的隐向量是用公式计算出来的。而深度学习的embedding是通过有监督学习的方法训练得到的。

对比embedding，一般的类型数据在模型处理的时候会使用onehot的方法，这样每个object对应的向量就类似于
[0,1,0,0,0,0,0] 这种方式使得数据非常的稀疏，且包含的信息量很少。 embedding则可以获得稠密的低纬
向量，而且embedding之间的距离可以很好地描述物体之间实际内在的关联
类似于 [0.23, 0.12] [0.33, 0.55] 这样的embedding向量。


# word2vec
为了理解embedding，从最常见的word2vec开始

## 训练模式
学习word2vec，需要先要知道NLP模型里面的skip-gram 和 CBOW 他们分别是word2vec的两种训练模式
简单来理解
CBOW 是continous bag of words， 通过上下文来预测当前值，相当于一句话扣掉一个词，让你来猜这个词是什么
"easyai 是 __ 人工 智能 网站"

Skip-gram 是用当前词来预测上下文，相当于给你一个词，让你猜他前面和后面可能出现什么词
__ __ 最好的 __ __

## 优化方法 两种加速方式
1. Negative Sample 负采样
2. Hierarchical Softmax

## 优缺点
优点： 
1. word2vec会考虑上下文，跟之前的embedding相比，效果更好，但不如18年以后的方法
2. 比之前的embedding方法纬度更少，速度更快
3. 通用性很强，可以用在各种NLP任务中

缺点：
1. 由于词和向量是一对一的关系，所以多义词的问题无法解决
2. word2vec是一种静态的方式，虽然通用型强，但是无法针对特定任务做动态优化






