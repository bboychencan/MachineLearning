{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import jieba, os, Levenshtein, time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse\n",
    "from utility import read_file, lcseque_lens, lcsubstr_lens, find_longest_prefix, printlog\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from xpinyin import Pinyin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run base_feature\n"
     ]
    }
   ],
   "source": [
    "print('run base_feature')\n",
    "# 配置信息\n",
    "is_print_output = True\n",
    "all_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "# 读入数据\n",
    "train_data = read_file('./data/oppo_round1_train_20180929.txt')\n",
    "val_data = read_file('./data/oppo_round1_vali_20180929.txt')\n",
    "test_data = read_file('./data/oppo_round1_test_A_20180929.txt', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed in 0m 11s\n"
     ]
    }
   ],
   "source": [
    "# 拼接数据一起做特征\n",
    "not_zip_all_data = pd.concat((train_data, val_data, test_data), axis=0, ignore_index=True, sort=False)\n",
    "time_elapsed = time.time() - since\n",
    "print('completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) # 打印出来时间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete in 0m 1s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "#Fix empty query_prediction\n",
    "not_zip_all_data.loc[not_zip_all_data['query_prediction'] == '', 'query_prediction'] = '{}'\n",
    "#Change label to int\n",
    "not_zip_all_data['label'] = not_zip_all_data['label'].astype('int')\n",
    "#Save to-drop columns\n",
    "drop_feature = []\n",
    "time_elapsed = time.time() - since\n",
    "print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) # 打印出来时间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete in 0m 6s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "not_zip_all_data['diction_label'] = encoder.fit_transform(not_zip_all_data.query_prediction)\n",
    "\n",
    "# Remove duplicate, calculate staticstics features\n",
    "all_data = not_zip_all_data.drop('label', axis = 1).drop_duplicates().reset_index(drop = True)\n",
    "drop_feature.append('diction_label')\n",
    "time_elapsed = time.time() - since\n",
    "print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) # 打印出来时间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete in 0m 13s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "def str_to_dict(dict_str):\n",
    "    my_dict = eval(dict_str)\n",
    "    keys, values = my_dict.keys(), my_dict.values()\n",
    "    my_dict = dict(zip(keys, list(map(lambda x: float(x), values))))\n",
    "    return my_dict\n",
    "\n",
    "all_data['query_prediction'] = all_data.query_prediction.apply(lambda x: str_to_dict(x))\n",
    "all_data['max_query_prediction_keys'] = all_data.query_prediction.apply(lambda x: '' if x == {} else max(x, key=x.get))\n",
    "all_data['query_prediction_keys'] = all_data.query_prediction.apply(lambda x: list(x.keys()))\n",
    "all_data['query_prediction_values'] = all_data.query_prediction.apply(lambda x: list(x.values()))\n",
    "\n",
    "drop_feature.extend(['query_prediction', 'query_prediction_keys', 'query_prediction_values', 'max_query_prediction_keys'])\n",
    "time_elapsed = time.time() - since\n",
    "print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) # 打印出来时间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.662 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete in 0m 39s\n"
     ]
    }
   ],
   "source": [
    "# 分词\n",
    "since = time.time()\n",
    "all_data['prefix_jieba'] = all_data.prefix.apply(lambda x: \" \".join(jieba.cut(x, cut_all = False)))\n",
    "all_data['prefix_jieba'] = all_data.prefix_jieba.apply(lambda x: \" \".join(x.split()))\n",
    "all_data['title_jieba'] = all_data.title.apply(lambda x: \" \".join(jieba.cut(x, cut_all = False)))\n",
    "all_data['title_jieba'] = all_data.title_jieba.apply(lambda x: \" \".join(x.split()))\n",
    "\n",
    "all_data['query_jieba'] = all_data.max_query_prediction_keys.apply(lambda x: \" \".join(jieba.cut(x, cut_all = False)))\n",
    "all_data['query_jieba'] = all_data.query_jieba.apply(lambda x: \" \".join(x.split()))\n",
    "drop_feature.extend(['prefix_jieba', 'title_jieba', 'query_jieba'])\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) # 打印出来时间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete in 0m 2s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "# 转换成拼音\n",
    "p = Pinyin()\n",
    "all_data['prefix_pinyin'] = all_data.prefix.apply(lambda x: p.get_pinyin(x, ' '))\n",
    "\n",
    "drop_feature.append('prefix_pinyin')\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) # 打印出来时间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete in 0m 2s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "# 去掉prefix、title中的空格，转换大小写\n",
    "all_data['prefix_fix'] = all_data.prefix.apply(lambda x: x.replace(' ', '').lower())\n",
    "all_data['title_fix'] = all_data.title.apply(lambda x: x.replace(' ', '').lower())\n",
    "all_data['query_fix'] = all_data.max_query_prediction_keys.apply(lambda x: x.replace(' ', '').lower())\n",
    "all_data['query_prediction_keys_fix'] = all_data.query_prediction_keys.apply(lambda x: list(map(lambda item: item.replace(' ', '').lower(), x)))\n",
    "\n",
    "drop_feature.extend(['prefix_fix', 'title_fix', 'query_fix', 'query_prediction_keys_fix'])\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) # 打印出来时间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算prefix长度\n",
      "计算title长度\n",
      "计算max_query_prediction_keys长度\n",
      "计算query_prediction_values长度\n",
      "complete in 0m 1s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "# ----- length 特征 -----\n",
    "list_length_feature = ['prefix', 'title', 'max_query_prediction_keys', 'query_prediction_values']\n",
    "\n",
    "for feature in list_length_feature:\n",
    "    printlog('计算' + feature + '长度', is_print_output)\n",
    "    all_data[feature + '_length'] = all_data[feature].apply(lambda x: len(x))\n",
    "for feature in ['prefix_jieba', 'title_jieba', 'query_jieba']:\n",
    "    all_data[feature + '_length'] = all_data[feature].apply(lambda x: len(x.split()))\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) # 打印出来时间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete in 0m 11s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "# ----- nunique 特征 -----\n",
    "list_nunique_feature = ['prefix', 'title', 'tag', 'max_query_prediction_keys', 'prefix_pinyin']\n",
    "\n",
    "all_data['prefix_nunique_title'] = all_data.groupby('prefix').title.transform('nunique')\n",
    "all_data['prefix_nunique_tag'] = all_data.groupby('prefix').tag.transform('nunique')\n",
    "\n",
    "all_data['title_nunique_prefix'] = all_data.groupby('title').prefix.transform('nunique')\n",
    "all_data['title_nunique_tag'] = all_data.groupby('title').tag.transform('nunique')\n",
    "all_data['title_nunique_query'] = all_data.groupby('title').max_query_prediction_keys.transform('nunique')\n",
    "all_data['title_nunique_prefix_pinyin'] = all_data.groupby('title').prefix_pinyin.transform('nunique')\n",
    "\n",
    "all_data['tag_nunique_prefix'] = all_data.groupby('tag').prefix.transform('nunique')\n",
    "all_data['tag_nunique_title'] = all_data.groupby('tag').title.transform('nunique')\n",
    "all_data['tag_nunique_max_query'] = all_data.groupby('tag').max_query_prediction_keys.transform('nunique')\n",
    "\n",
    "all_data['query_nunique_prefix'] = all_data.groupby('max_query_prediction_keys').prefix.transform('nunique')\n",
    "all_data['query_nunique_title'] = all_data.groupby('max_query_prediction_keys').title.transform('nunique')\n",
    "all_data['query_nunique_tag'] = all_data.groupby('max_query_prediction_keys').tag.transform('nunique')\n",
    "all_data['query_nunique_prefix_pinyin'] = all_data.groupby('max_query_prediction_keys').prefix_pinyin.transform('nunique')\n",
    "\n",
    "all_data['prefix_pinyin_nunique_prefix'] = all_data.groupby('prefix_pinyin').prefix.transform('nunique')\n",
    "all_data['prefix_pinyin_nunique_title'] = all_data.groupby('prefix_pinyin').title.transform('nunique')\n",
    "all_data['prefix_pinyin_nunique_tag'] = all_data.groupby('prefix_pinyin').tag.transform('nunique')\n",
    "all_data['prefix_pinyin_nunique_query'] = all_data.groupby('prefix_pinyin').max_query_prediction_keys.transform('nunique')\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) # 打印出来时间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete in 0m 37s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "# is in feature\n",
    "all_data['prefix_isin_title'] = all_data.apply(lambda row:1 if row['prefix_fix'] in row['title_fix'] else 0, axis = 1)\n",
    "all_data['tag_isin_title'] = all_data.apply(lambda row:1 if row['tag'] in row['title_fix'] else 0, axis = 1)\n",
    "all_data['query_isin_title'] = all_data.apply(lambda row:1 if row['query_fix'] in row['title_fix'] else 0, axis = 1)\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) # 打印出来时间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_func = [Levenshtein.ratio, Levenshtein.distance, lcsubstr_lens, lcseque_lens]\n",
    "statistics_func = [max, min, np.mean, np.std]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算prefix_fix与query_prediction_keys_ratio相似度的list\n",
      "计算prefix_fix与query_prediction_keys_distance相似度的list\n",
      "计算prefix_fix与query_prediction_keys_lcsubstr_lens相似度的list\n",
      "计算prefix_fix与query_prediction_keys_lcseque_lens相似度的list\n",
      "计算title_fix与query_prediction_keys_ratio相似度的list\n",
      "计算title_fix与query_prediction_keys_distance相似度的list\n",
      "计算title_fix与query_prediction_keys_lcsubstr_lens相似度的list\n",
      "计算title_fix与query_prediction_keys_lcseque_lens相似度的list\n",
      "complete in 12m 46s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "# 计算prefix/title与query_prediction_keys相似度的list\n",
    "list_with_query_prediction_keys_similarity = ['prefix_fix', 'title_fix']\n",
    "for feature in list_with_query_prediction_keys_similarity:\n",
    "    for func in similarity_func:\n",
    "        printlog('计算' + feature + '与query_prediction_keys_' + func.__name__  + '相似度的list', is_print_output)\n",
    "        all_data[feature + '_query_prediction_keys_' + func.__name__ +  '_list'] = all_data.apply(lambda row: [func(query, row[feature]) for query in row['query_prediction_keys_fix']], axis = 1)\n",
    "        drop_feature.append(feature + '_query_prediction_keys_' + func.__name__ + '_list')\n",
    "        \n",
    "time_elapsed = time.time() - since\n",
    "print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) # 打印出来时间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算prefix_fix与query_prediction_values_ratio相似度的list的乘积list\n",
      "计算prefix_fix与query_prediction_values_distance相似度的list的乘积list\n",
      "计算prefix_fix与query_prediction_values_lcsubstr_lens相似度的list的乘积list\n",
      "计算prefix_fix与query_prediction_values_lcseque_lens相似度的list的乘积list\n",
      "计算title_fix与query_prediction_values_ratio相似度的list的乘积list\n",
      "计算title_fix与query_prediction_values_distance相似度的list的乘积list\n",
      "计算title_fix与query_prediction_values_lcsubstr_lens相似度的list的乘积list\n",
      "计算title_fix与query_prediction_values_lcseque_lens相似度的list的乘积list\n",
      "complete in 2m 34s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "# 计算prefix/title与query_prediction_keys相似度的list与query_prediction_values list的乘积list\n",
    "list_with_query_prediction_keys_similarity_multiple = ['prefix_fix', 'title_fix']\n",
    "multiple_similarity_func = [Levenshtein.ratio, Levenshtein.distance, lcsubstr_lens, lcseque_lens]\n",
    "for feature in list_with_query_prediction_keys_similarity_multiple:\n",
    "    for multiple_func in multiple_similarity_func:\n",
    "        printlog('计算' + feature + '与query_prediction_values_' + multiple_func.__name__  + '相似度的list的乘积list', is_print_output)\n",
    "        all_data[feature +  '_query_prediction_values_mutiple_' + multiple_func.__name__ + '_list'] = all_data.apply(lambda row: list(map(lambda x, y: x * y, row[feature + '_query_prediction_keys_' + multiple_func.__name__ +  '_list'], row['query_prediction_values'])), axis = 1)\n",
    "        drop_feature.append(feature +  '_query_prediction_values_mutiple_' + multiple_func.__name__ + '_list')\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) # 打印出来时间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算prefix_fix_query_prediction_keys_ratio_list的max\n",
      "计算prefix_fix_query_prediction_keys_ratio_list的min\n",
      "计算prefix_fix_query_prediction_keys_ratio_list的mean\n",
      "计算prefix_fix_query_prediction_keys_ratio_list的std\n",
      "计算prefix_fix_query_prediction_keys_distance_list的max\n",
      "计算prefix_fix_query_prediction_keys_distance_list的min\n",
      "计算prefix_fix_query_prediction_keys_distance_list的mean\n",
      "计算prefix_fix_query_prediction_keys_distance_list的std\n",
      "计算prefix_fix_query_prediction_keys_lcsubstr_lens_list的max\n",
      "计算prefix_fix_query_prediction_keys_lcsubstr_lens_list的min\n",
      "计算prefix_fix_query_prediction_keys_lcsubstr_lens_list的mean\n",
      "计算prefix_fix_query_prediction_keys_lcsubstr_lens_list的std\n",
      "计算prefix_fix_query_prediction_keys_lcseque_lens_list的max\n",
      "计算prefix_fix_query_prediction_keys_lcseque_lens_list的min\n",
      "计算prefix_fix_query_prediction_keys_lcseque_lens_list的mean\n",
      "计算prefix_fix_query_prediction_keys_lcseque_lens_list的std\n",
      "计算title_fix_query_prediction_keys_ratio_list的max\n",
      "计算title_fix_query_prediction_keys_ratio_list的min\n",
      "计算title_fix_query_prediction_keys_ratio_list的mean\n",
      "计算title_fix_query_prediction_keys_ratio_list的std\n",
      "计算title_fix_query_prediction_keys_distance_list的max\n",
      "计算title_fix_query_prediction_keys_distance_list的min\n",
      "计算title_fix_query_prediction_keys_distance_list的mean\n",
      "计算title_fix_query_prediction_keys_distance_list的std\n",
      "计算title_fix_query_prediction_keys_lcsubstr_lens_list的max\n",
      "计算title_fix_query_prediction_keys_lcsubstr_lens_list的min\n",
      "计算title_fix_query_prediction_keys_lcsubstr_lens_list的mean\n",
      "计算title_fix_query_prediction_keys_lcsubstr_lens_list的std\n",
      "计算title_fix_query_prediction_keys_lcseque_lens_list的max\n",
      "计算title_fix_query_prediction_keys_lcseque_lens_list的min\n",
      "计算title_fix_query_prediction_keys_lcseque_lens_list的mean\n",
      "计算title_fix_query_prediction_keys_lcseque_lens_list的std\n",
      "计算prefix_fix_query_prediction_values_mutiple_ratio_list的max\n",
      "计算prefix_fix_query_prediction_values_mutiple_ratio_list的min\n",
      "计算prefix_fix_query_prediction_values_mutiple_ratio_list的mean\n",
      "计算prefix_fix_query_prediction_values_mutiple_ratio_list的std\n",
      "计算prefix_fix_query_prediction_values_mutiple_distance_list的max\n",
      "计算prefix_fix_query_prediction_values_mutiple_distance_list的min\n",
      "计算prefix_fix_query_prediction_values_mutiple_distance_list的mean\n",
      "计算prefix_fix_query_prediction_values_mutiple_distance_list的std\n",
      "计算prefix_fix_query_prediction_values_mutiple_lcsubstr_lens_list的max\n",
      "计算prefix_fix_query_prediction_values_mutiple_lcsubstr_lens_list的min\n",
      "计算prefix_fix_query_prediction_values_mutiple_lcsubstr_lens_list的mean\n",
      "计算prefix_fix_query_prediction_values_mutiple_lcsubstr_lens_list的std\n",
      "计算prefix_fix_query_prediction_values_mutiple_lcseque_lens_list的max\n",
      "计算prefix_fix_query_prediction_values_mutiple_lcseque_lens_list的min\n",
      "计算prefix_fix_query_prediction_values_mutiple_lcseque_lens_list的mean\n",
      "计算prefix_fix_query_prediction_values_mutiple_lcseque_lens_list的std\n",
      "计算title_fix_query_prediction_values_mutiple_ratio_list的max\n",
      "计算title_fix_query_prediction_values_mutiple_ratio_list的min\n",
      "计算title_fix_query_prediction_values_mutiple_ratio_list的mean\n",
      "计算title_fix_query_prediction_values_mutiple_ratio_list的std\n",
      "计算title_fix_query_prediction_values_mutiple_distance_list的max\n",
      "计算title_fix_query_prediction_values_mutiple_distance_list的min\n",
      "计算title_fix_query_prediction_values_mutiple_distance_list的mean\n",
      "计算title_fix_query_prediction_values_mutiple_distance_list的std\n",
      "计算title_fix_query_prediction_values_mutiple_lcsubstr_lens_list的max\n",
      "计算title_fix_query_prediction_values_mutiple_lcsubstr_lens_list的min\n",
      "计算title_fix_query_prediction_values_mutiple_lcsubstr_lens_list的mean\n",
      "计算title_fix_query_prediction_values_mutiple_lcsubstr_lens_list的std\n",
      "计算title_fix_query_prediction_values_mutiple_lcseque_lens_list的max\n",
      "计算title_fix_query_prediction_values_mutiple_lcseque_lens_list的min\n",
      "计算title_fix_query_prediction_values_mutiple_lcseque_lens_list的mean\n",
      "计算title_fix_query_prediction_values_mutiple_lcseque_lens_list的std\n",
      "计算query_prediction_values的max\n",
      "计算query_prediction_values的min\n",
      "计算query_prediction_values的mean\n",
      "计算query_prediction_values的std\n",
      "complete in 6m 9s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "# 所有list相关统计的特征\n",
    "# 找出所有list的特征\n",
    "list_feature = list(filter(lambda x: x.find('list') != -1, drop_feature)) + ['query_prediction_values']\n",
    "for feature in list_feature:\n",
    "    for statistics in statistics_func:\n",
    "        printlog('计算' + feature + '的' + statistics.__name__, is_print_output)\n",
    "        all_data[feature + '_' + statistics.__name__] = all_data[feature].apply(lambda x: statistics(x) if x else np.nan)\n",
    "        \n",
    "time_elapsed = time.time() - since\n",
    "print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) # 打印出来时间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算prefix_fix与title_fix的ratio相似度\n",
      "计算prefix_fix与title_fix的distance相似度\n",
      "计算prefix_fix与title_fix的lcsubstr_lens相似度\n",
      "计算prefix_fix与title_fix的lcseque_lens相似度\n",
      "计算prefix_fix与query_fix的ratio相似度\n",
      "计算prefix_fix与query_fix的distance相似度\n",
      "计算prefix_fix与query_fix的lcsubstr_lens相似度\n",
      "计算prefix_fix与query_fix的lcseque_lens相似度\n",
      "计算title_fix与query_fix的ratio相似度\n",
      "计算title_fix与query_fix的distance相似度\n",
      "计算title_fix与query_fix的lcsubstr_lens相似度\n",
      "计算title_fix与query_fix的lcseque_lens相似度\n",
      "complete in 5m 22s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "# 计算prefix/title/max_query_prediction_keys间的相似度\n",
    "list_single_feature = ['prefix_fix', 'title_fix', 'query_fix']\n",
    "\n",
    "for times in range(len(list_single_feature)):\n",
    "    first_feature = list_single_feature.pop(0)\n",
    "    for second_feature in list_single_feature:\n",
    "        for func in similarity_func:\n",
    "            printlog('计算' + first_feature + '与' + second_feature + '的' + func.__name__ + '相似度', is_print_output)\n",
    "            all_data[func.__name__ + '_similarity_' + first_feature + '_with_' + second_feature] = all_data.apply(lambda row: func(row[first_feature], row[second_feature]), axis = 1)\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) # 打印出来时间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "# 拼接回原数据\n",
    "all_data = all_data.drop('query_prediction', axis = 1)\n",
    "not_zip_all_data = pd.merge(not_zip_all_data, all_data, how = 'left', on = ['prefix', 'title', 'tag', 'diction_label'])\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) # 打印出来时间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算某特征单次点击\n",
    "for feature in list_click_feature:\n",
    "    printlog('计算' + feature + '点击次数', is_print_output)\n",
    "    not_zip_all_data[feature + '_click'] = not_zip_all_data.groupby(feature)[feature].transform('count')\n",
    "# 部分二元交叉点击\n",
    "not_zip_all_data['prefix_title_click'] = not_zip_all_data.groupby(['prefix', 'title']).prefix.transform('count')\n",
    "not_zip_all_data['prefix_tag_click'] = not_zip_all_data.groupby(['prefix', 'tag']).prefix.transform('count')\n",
    "not_zip_all_data['title_tag_click'] = not_zip_all_data.groupby(['title', 'tag']).title.transform('count')\n",
    "not_zip_all_data['title_max_query_prediction_keys_click'] = not_zip_all_data.groupby(['title', 'max_query_prediction_keys']).title.transform('count')\n",
    "not_zip_all_data['tag_max_query_prediction_keys_click'] = not_zip_all_data.groupby(['tag', 'max_query_prediction_keys']).tag.transform('count')\n",
    "# 部分三元交叉点击\n",
    "not_zip_all_data['prefix_title_tag_click'] = not_zip_all_data.groupby(['prefix', 'title', 'tag']).prefix.transform('count')\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) # 打印出来时间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "# 转换tag\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "not_zip_all_data['tag'] = encoder.fit_transform(not_zip_all_data.tag)\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "not_zip_all_data['prefix'] = encoder.fit_transform(not_zip_all_data.prefix)\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "not_zip_all_data['title'] = encoder.fit_transform(not_zip_all_data.title)\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) # 打印出来时间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存数据\n",
    "not_zip_all_data.drop(drop_feature, axis = 1).to_csv('./data/base_0610.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "time_elapsed = time.time() - all_start_time\n",
    "print('final complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) # 打印出来时间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
