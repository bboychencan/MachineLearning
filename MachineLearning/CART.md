# 分类树

CART

classification and regression tree, 1984年提出的决策树学习方法。

CART假设决策树是二叉树，内部节点表示的是对数据根据某个特征的某个值的划分，特征小于或等于该划分值得数据分配在该内部节点的左节点，大于划分值的数据则被划分到右节点。

一般，我们对于不确定性的度量方法有信息熵(使用信息增益最大的划分方法作为划分依据)、基尼指数等，对于损失值可以采用平方损失函数等

-- 2021/05/10 --
CART被问到了，里面关于连续变量的处理方式不沟通清楚，总结一下

对于连续变量，在数据集中，也是离散的，因为数据样本量是有限的，所以最简单的方法可以在样本集内按照该连续变量进行排序，然后一次选取相邻点的均值作为分割边界，将数据集分为两份。然后从所有的方法中选择最好的划分。

对于离散类型特征，同样是二分，每次取特征的一个取值，将等于和不等于这个特征取值分成左右两份数据集

- 如果是回归树
每个输入X，都可以划分到对应的叶子节点，取该节点的平均值作为预测值
回归树的损失使用均方误差，在分割的时候，考虑最小均方误差的分割方法。最小化min (min(Sum(y-c1)^2) + min(Sum(y-c2)^2))
- 如果是分类树
每次分类的损失采用基尼指数，选择最小基尼增益的特征做划分。

跟ID3 C4.5的区别，特征可以重复使用。


## 提升树

提升树是以分类树或回归树为基分类器。基本idea是，第一个回归树预测效果可能一般，但是第二个回归树把第一个预测错的残差作为输入。
