# 分类树

CART

classification and regression tree, 1984年提出的决策树学习方法。

CART假设决策树是二叉树，内部节点表示的是对数据根据某个特征的某个值的划分，特征小于或等于该划分值得数据分配在该内部节点的左节点，大于划分值的数据则被划分到右节点。

一般，我们对于不确定性的度量方法有信息熵(使用信息增益最大的划分方法作为划分依据)、基尼指数等，对于损失值可以采用平方损失函数等

-- 2021/05/10 --
CART被问到了，里面关于连续变量的处理方式不沟通清楚，总结一下

对于连续变量，在数据集中，也是离散的，因为数据样本量是有限的，所以最简单的方法可以在样本集内按照该连续变量进行排序，然后一次选取相邻点的均值作为分割边界，将数据集分为两份。然后从所有的方法中选择最好的划分。

## 提升树

提升树是以分类树或回归树为基分类器。基本idea是，第一个回归树预测效果可能一般，但是第二个回归树把第一个预测错的残差作为输入。
