# Embedding


2021-12-19
embedding的概念在深度学习中可以说是最基础，最重要的概念。一直以来没有认真仔细地学习过这个，今天要研究清楚这个概念。


简单来讲，embedding就是用一个低纬的**数字**向量来表达object，非常类似推荐系统里面的矩阵分解算法给每一个user或者item
计算一个隐向量。 然后用这个隐向量来计算user或者item之间的距离。

这一步解决的是”将现实问题转化为数学问题“，是人工智能非常关键的一步。


矩阵分解里面的隐向量是用公式计算出来的。而深度学习的embedding是通过有监督学习的方法训练得到的。

对比embedding，一般的类型数据在模型处理的时候会使用onehot的方法，这样每个object对应的向量就类似于
[0,1,0,0,0,0,0] 这种方式使得数据非常的稀疏，且包含的信息量很少。 embedding则可以获得稠密的低纬
向量，而且embedding之间的距离可以很好地描述物体之间实际内在的关联
类似于 [0.23, 0.12] [0.33, 0.55] 这样的embedding向量。


# word2vec
为了理解embedding，从最常见的word2vec开始

## 训练模式
学习word2vec，需要先要知道NLP模型里面的skip-gram 和 CBOW 他们分别是word2vec的两种训练模式
简单来理解
CBOW 是continous bag of words， 通过上下文来预测当前值，相当于一句话扣掉一个词，让你来猜这个词是什么
"easyai 是 __ 人工 智能 网站"

Skip-gram 是用当前词来预测上下文，相当于给你一个词，让你猜他前面和后面可能出现什么词
__ __ 最好的 __ __

## 优化方法 两种加速方式
1. Negative Sample 负采样
2. Hierarchical Softmax

## 优缺点
优点： 
1. word2vec会考虑上下文，跟之前的embedding相比，效果更好，但不如18年以后的方法
2. 比之前的embedding方法纬度更少，速度更快
3. 通用性很强，可以用在各种NLP任务中

缺点：
1. 由于词和向量是一对一的关系，所以多义词的问题无法解决
2. word2vec是一种静态的方式，虽然通用型强，但是无法针对特定任务做动态优化


2021-12-20
# 了解word2vec需要涉及的知识点很多，这里一一攻破学习

## Huffman编码
这个知识点从最开始学习数据结构算法，接触了无数遍，但是可能因为很少手动coding这个算法，所以导致一直
记忆的很不深刻，这也说明了，最好的记忆方法还是动手练习。

又看了一边，原理依然是很简单，就是对有n个词，每个词的出现频率是Fi，现在需要找到一个编码方式，使得最终的
编码长度最短。
1. 第一步，先将问题转化成二叉树最短加权路径的问题，每个词代表着树上的一个叶子节点，每个节点都有一个权重。
那么最终的问题转化成了构造一个最短加权路径和的树。
2. 第二，这里面涉及到一个编码的知识点，对于定长的编码方式，只需要用固定长度即可，在解码的时候每次取固定长度的码
即可。 但是这种固定长度的编码一般来说不是最优的编码方式，例其实等价的某个词and 出现频率非常高，但是每次都要占用
固定的长度编码，很浪费存储。 为了实现编码长度的优化，肯定需要使用变长的编码方式。
既然编码长度是不固定的，那么在解码的时候就需要有方法来判断编码的边界。这里面用到的就是**前缀编码**
即，**任意字符的编码都不是其他字符的编码的前缀**，这样就避免了解码的时候产生歧义。

其实等价的，放在二叉树上来看，就是所有的字符都在叶子节点上，即可
3. 最后，就是如何构造一个二叉树，每个叶子节点的权重是出现频率，保证所有叶子节点的加权路径和最小。
构造方法是一个贪心算法，构造过程很简单，但是跟所有的贪心算法一样，证明他是最优的比较难。
这里只讲构造过程
- 构造一个所有树的集合，初始状态，有n个单节点的树，即所有的字符节点
- 每次从集合里挑选根节点权值最小的两颗树合并成一个树，树的根节点权重修改为两边子树根节点权重和。
- 删除刚才选择的两颗树，将新的树加入集合
- 重复上述2，3步骤直到最后只剩一棵树


## N-gram 模型
总结来讲，n-gram模型是这样一种模型，主要工作是在语料中统计各种词串出现的次数以及平滑化处理。
概率值计算好之后就存储起来，下次需要计算一个句子的概率时，只需找到相关的概率参数，将它们连乘起来就好了。




