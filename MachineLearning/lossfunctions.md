# Loss Functions

## Hinge Loss (外形很像hinge，中文里面的合页)

SVM中使用的损失函数

L(y) = max(0, 1 - y * t) 
t是目标值（1,-1), y是预测值 。 
- 当预测结果错误的时候，y * t为负数，L(yi) 返回 1 - y * t，与y值成线性关系
- 当预测正确且|y| >= 1时，损失为0

## Classification Error 分类错误率


## Cross Entropy Loss
什么是交叉熵损失函数，一种用于分类的损失函数。
看了一下，其实就是逻辑回归里面用到的损失函数，其实就是最大似然函数取log的形式。但是名字是怎么来的呢？

信息量的定义 I(x)=−log(P(x)) ， 直观来看，发生的概率越低，信息量区域正无穷，发生概率为1，信息量为0.

信息熵也被称为熵，用来表示所有信息量的期望。

交叉熵，用来高衡量在给定的真实分布下，使用非真实分布指定的策略消除系统的不确定性所需要付出努力的大小

在逻辑回归中
因此Hessian矩阵半正定，目标函数是凸函数。logistic回归求解的优化问题是不带约束条件的凸优化问题。可以证明，如果使用欧氏距离作为目标函数则无法保证目标函数是凸函数。这是使用交叉熵而不使用欧氏距离作为logistic回归的目标函数的主要原因。

相对熵=交叉熵−信息熵

## Softmax Loss

## MSE

## Exponential Loss